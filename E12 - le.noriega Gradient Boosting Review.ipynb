{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E12 - Gradient Boosting Review\n",
    "\n",
    "Search for and comment about the main differences between the algorithms implemented in: \n",
    "\n",
    "(1) [ Gradient Boosting Classifier ](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "\n",
    "(2) [ XGB Classifier ](https://xgboost.readthedocs.io/en/latest/python/python_intro.html)\n",
    "\n",
    "Write at least 300 words explaining the difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\t\n",
    "El aumento en el gradiente funciona a través de la construcción de árboles de forma serial, en donde cada árbol intenta corregir los errores del árbol anterior.\n",
    "\n",
    "Por defecto no hay aleatorización en los árboles de regresión potenciados por gradiente, y en su lugar se utiliza una poda previa.\n",
    "\n",
    "Generalmente utilizan árboles muy poco profundos, entre uno y cinco, lo que hace que el modelo sea más pequeño en términos de memoria y haga predicciones más rápidas.\n",
    "\n",
    "Cada árbol proporciona buenas predicciones sobre una parte de los datos, por lo que en la medida en que se agregan más y más árboles se mejorará de manera iterativa el rendimiento.\n",
    "\n",
    "Son ampliamente utilizados en competiciones de aprendizaje automático y son ampliamente utilizados en la industria.\n",
    "\n",
    "Son ampliamente más sensibles a la configuración que en el método de random forests pero pueden proporcionar una mejor precisión si se configuran correctamente.\n",
    "\n",
    "Para medir la precisión del modelo no solo se tiene en cuenta la poda previa y la cantidad de árboles, sino que incluye la tasa de aprendizaje que controla la intensidad con la que cada árbol intenta corregir los errores de los anteriores. A mayor tasa de aprendizaje son más fuertes las correcciones que puede hacer a los otros árboles.\n",
    "\n",
    "Agregar más árboles al conjunto, lo que puede lograrse aumentando n_estimadores, también aumenta la complejidad del modelo, ya que el modelo tiene más posibilidades de corregir errores en el conjunto de entrenamiento.\n",
    "\n",
    "Se encuentran entre los modelos más poderosos y más utilizados para el aprendizaje supervisado. Funcina bien en sin escalar y mezclando cracterísticas binarias y continuas, aunque al igual que otros modelos basados en árboles no funcina bien en datos dispersos de alta dimensión.\n",
    "\n",
    "En contraste con los bosques aleatorios, donde un mayor valor de n_estimators siempre es mejor, el aumento de n_estimadores en el aumento de gradiente conduce a un modelo más complejo, lo que puede llevar a un sobreajuste.\n",
    "\n",
    "Si desea aplicar un aumento de gradiente a un problema a gran escala, podría valer la pena examinar el paquete xgboost, que en el momento de la escritura es más rápida (y, a veces, más fácil de ajustar) que el scikit-learn Implementación del aumento de gradiente en muchos conjuntos de datos.\n",
    "\n",
    "El XGBoost classifier es un Gradient Boosting Extremo. Se utiliza para problemas de aprendizaje supervisado. \n",
    "Es una implementación avanzada del algoritmo de aumento del gradiente, en la que se realiza un procesamiento en paralelo lo que lo hace más rápido que el GB. Permite a los usuarios definir objetivos de optimización personalizados y diversos criterios de evaluación.\n",
    "\n",
    "Los parámetros generales se dividen en tre categorìas: generales, de refuerzo y de aprendizaje.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
